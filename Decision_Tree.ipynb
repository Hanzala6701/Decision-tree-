{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5NRi+P17y8UQw5VnK8Tgs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hanzala6701/Decision-tree-/blob/main/Decision_Tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "- A Decision Tree is a supervised machine learning algorithm that models decisions or classifications as a tree-like flowchart, branching from a root node into internal nodes (conditions) and terminal leaf nodes (final class labels). It classifies data by splitting it into homogeneous subsets based on feature values using criteria like Gini impurity or Information Gain.\n",
        "\n",
        "i)  Structure: It consists of a root node (start), branches (outcomes of decisions), internal nodes (attribute tests), and leaf nodes (predicted class).\n",
        "\n",
        "ii) Process: The algorithm starts at the root and poses questions about the data features, splitting the dataset based on the answer.\n",
        "\n",
        "iii) Splitting: It recursively partitions the data, choosing features that best separate the classes to minimize uncertainty (using metrics like Gini or Entropy).\n",
        "\n",
        "iv) Stopping Criteria: The splitting continues until the node becomes \"pure\" (all samples belong to one class) or another stopping criterion, such as maximum depth, is met.\n",
        "\n",
        "V) Result: A new, unseen data point follows the path of decisions to a leaf node, which assigns the final classification."
      ],
      "metadata": {
        "id": "4TdQ7YrQuQjx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        " -  Gini Impurity and Entropy are metrics used to measure node impurity (mixture of classes) in Decision Trees to determine the best feature split. Gini measures the probability of incorrect classification (\\(1-\\sum p_{i}^{2}\\)), while Entropy measures disorder or information uncertainty (\\(-\\sum p_{i}\\log _{2}(p_{i})\\)). Both drive the algorithm to create pure nodes (impurity=0) by selecting splits that maximize information gain.\n",
        "\n",
        "- Gini Impurity and Entropy Concepts\n",
        "\n",
        "i) Gini Impurity: Measures the frequency with which a random element would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. It is generally faster to calculate because it does not involve logarithmic functions. It is the default metric used by the CART algorithm.\n",
        "\n",
        "ii) Entropy: A measure of disorder or uncertainty in the node, ranging from 0 (perfectly pure) to 1 (50/50 split in binary classification). It is used to quantify the information gain, aiming to reduce the uncertainty.\n",
        "\n",
        "- Impact on Decision Tree Splits\n",
        "\n",
        "i) Selection Criterion: The decision tree algorithm calculates the impurity of child nodes for all possible splits.\n",
        "\n",
        "ii) Minimizing Impurity: The algorithm selects the feature split that results in the lowest weighted Gini Impurity or lowest Entropy (highest Information Gain).\n",
        "\n",
        "iii) Resulting Structure: Both metrics guide the tree to make splits that increase the homogeneity of the nodes."
      ],
      "metadata": {
        "id": "Pe496YJaueOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "-   Information Gain (IG) in decision trees measures the reduction in entropy (impurity or uncertainty) within a dataset after it is split based on a specific attribute. It is calculated as the difference between the parent node's entropy and the weighted average of the child nodes' entropy. High IG indicates that a feature effectively organizes data into purer subsets.\n",
        "\n",
        "- Why Information Gain is Important for Splitting\n",
        "\n",
        "i) Optimal Splitting: It identifies the attribute that best separates data into distinct classes, creating the most informative split at each node.\n",
        "\n",
        "ii) Reduced Uncertainty: By choosing the feature with the highest Information Gain, the model minimizes impurity (maximum homogeneity) in the resulting child nodes.\n",
        "\n",
        "iii) Decision Tree Construction: It provides a mathematical, objective criterion for choosing the best split, essential for building efficient tree algorithms like ID3, C4.5, and CART.\n",
        "\n",
        "- Key Concepts Entropy (\\(H\\)): Measures the impurity or randomness of a node. \\(H=0\\) indicates a pure node (all samples belong to one class), while higher values indicate higher uncertainty."
      ],
      "metadata": {
        "id": "a6clIDQwvXpF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "AidEavaCrqgw",
        "outputId": "9a5fee77-24d4-4bdf-ba87-a98b96b1ed14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "\n",
            "Feature Importances:\n",
            "- sepal length (cm): 0.00\n",
            "- sepal width (cm): 0.02\n",
            "- petal length (cm): 0.89\n",
            "- petal width (cm): 0.09\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nModel Accuracy: 1.00\\n\\nFeature Importances:\\n- sepal length (cm): 0.00\\n- sepal width (cm): 0.00\\n- petal length (cm): 0.94\\n- petal width (cm): 0.06\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "\n",
        "\"\"\"6) Write a Python program to:\n",
        "\n",
        "Load the Iris Dataset\n",
        "\n",
        "Train a Decision Tree Classifier using the Gini criterion\n",
        "\n",
        "Print the model's accuracy and feature importances\n",
        "\n",
        "(Include your Python code and output in the code box below.)\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "# We use a fixed random state for reproducibility\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 2. Train a Decision Tree Classifier using the Gini criterion\n",
        "# The 'criterion' parameter is set to 'gini' by default, but explicitly included for clarity\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 3. Print the model's accuracy and feature importances\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\\n\")\n",
        "\n",
        "# Print the feature importances\n",
        "print(\"Feature Importances:\")\n",
        "importances = model.feature_importances_\n",
        "for name, importance in zip(feature_names, importances):\n",
        "    print(f\"- {name}: {importance:.2f}\")\n",
        "\n",
        "# Example of how the output will look when run:\n",
        "\"\"\"\n",
        "Model Accuracy: 1.00\n",
        "\n",
        "Feature Importances:\n",
        "- sepal length (cm): 0.00\n",
        "- sepal width (cm): 0.00\n",
        "- petal length (cm): 0.94\n",
        "- petal width (cm): 0.06\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" 7) Write a Python program to:\n",
        "\n",
        "Load the Iris Dataset\n",
        "\n",
        "Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.\"\"\"\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def train_and_compare_decision_trees():\n",
        "    \"\"\"\n",
        "    Loads the Iris dataset, trains two Decision Tree classifiers (one with max_depth=3 and one fully grown),\n",
        "    and compares their accuracies.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Load the Iris Dataset\n",
        "    # The dataset is loaded as a Bunch object, a dictionary-like container\n",
        "    iris = load_iris()\n",
        "    X = iris.data  # Features\n",
        "    y = iris.target # Target labels\n",
        "\n",
        "    # 2. Split the dataset into training and testing sets\n",
        "    # We use a 70/30 split for demonstration\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    # 3. Train a Decision Tree Classifier with max_depth=3\n",
        "    # Setting random_state ensures reproducibility\n",
        "    tree_depth_3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "    tree_depth_3.fit(X_train, y_train)\n",
        "\n",
        "    # 4. Train a fully-grown tree (default behavior of DecisionTreeClassifier is to grow until all leaves are pure or all leaves contain less than min_samples_split samples)\n",
        "    full_tree = DecisionTreeClassifier(random_state=42)\n",
        "    full_tree.fit(X_train, y_train)\n",
        "\n",
        "    # 5. Make predictions\n",
        "    y_pred_depth_3 = tree_depth_3.predict(X_test)\n",
        "    y_pred_full_tree = full_tree.predict(X_test)\n",
        "\n",
        "    # 6. Compare Accuracies\n",
        "    accuracy_depth_3 = accuracy_score(y_test, y_pred_depth_3)\n",
        "    accuracy_full_tree = accuracy_score(y_test, y_pred_full_tree)\n",
        "\n",
        "    print(f\"Accuracy of the tree with max_depth=3: {accuracy_depth_3:.4f}\")\n",
        "    print(f\"Accuracy of the fully-grown tree: {accuracy_full_tree:.4f}\")\n",
        "\n",
        "    # Optional: print the actual depth of the fully grown tree\n",
        "    print(f\"Actual depth of the fully-grown tree: {full_tree.tree_.max_depth}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_compare_decision_trees()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbIB1gwQsLJB",
        "outputId": "0ea0c1f4-d998-4ca7-8ff6-191e32a3b21d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the tree with max_depth=3: 1.0000\n",
            "Accuracy of the fully-grown tree: 1.0000\n",
            "Actual depth of the fully-grown tree: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"8) Write a Python program to:\n",
        "\n",
        "Load the California Housing dataset from sklearn\n",
        "\n",
        "Train a Decision Tree Regressor\n",
        "\n",
        "Print the Mean Squared Error (MSE) and feature importances.\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Load the California Housing dataset\n",
        "print(\"Loading California Housing dataset...\")\n",
        "# Fetching the dataset returns a Bunch object, which behaves like a dictionary\n",
        "california_housing = fetch_california_housing(as_frame=True)\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = california_housing.data\n",
        "y = california_housing.target\n",
        "feature_names = california_housing.feature_names\n",
        "\n",
        "print(f\"Dataset loaded. X shape: {X.shape}, y shape: {y.shape}\")\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(f\"Data split into training ({X_train.shape[0]} samples) and testing ({X_test.shape[0]} samples) sets.\")\n",
        "\n",
        "# 2. Train a Decision Tree Regressor\n",
        "print(\"\\nTraining a Decision Tree Regressor...\")\n",
        "# Initialize the regressor\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "dt_regressor.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# 3. Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = dt_regressor.predict(X_test)\n",
        "\n",
        "# Calculate the Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"\\nMean Squared Error (MSE) on test set: {mse:.4f}\")\n",
        "\n",
        "# Get and print feature importances\n",
        "# The feature_importances_ attribute returns the importance scores\n",
        "importances = dt_regressor.feature_importances_\n",
        "\n",
        "# Create a pandas Series for better visualization\n",
        "feature_importance_series = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importance_series)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEsytkoHsu0v",
        "outputId": "0d650846-1842-4d21-fb99-0ba1f0fefefb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading California Housing dataset...\n",
            "Dataset loaded. X shape: (20640, 8), y shape: (20640,)\n",
            "Data split into training (16512 samples) and testing (4128 samples) sets.\n",
            "\n",
            "Training a Decision Tree Regressor...\n",
            "Model training complete.\n",
            "\n",
            "Mean Squared Error (MSE) on test set: 0.4952\n",
            "\n",
            "Feature Importances:\n",
            "MedInc        0.528509\n",
            "AveOccup      0.130838\n",
            "Latitude      0.093717\n",
            "Longitude     0.082902\n",
            "AveRooms      0.052975\n",
            "HouseAge      0.051884\n",
            "Population    0.030516\n",
            "AveBedrms     0.028660\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" 9) Write a Python program to:\n",
        "\n",
        "Load the Iris Dataset\n",
        "\n",
        "Tune the Decision Tree's max depth and min_samples_split using\n",
        "\n",
        "GridSearchCV\n",
        "\n",
        "Print the best parameters and the resulting model accuracy\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable\n",
        "\n",
        "# Split the data into training and testing sets (optional but good practice)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 2. Tune the Decision Tree's max depth and min_samples_split using GridSearchCV\n",
        "# Define the Decision Tree model\n",
        "dtc = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define the grid of hyperparameters to search\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, 6],\n",
        "    'min_samples_split': [2, 3, 4, 5, 10]\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "# cv=5 means 5-fold cross-validation is used\n",
        "grid_search = GridSearchCV(estimator=dtc, param_grid=param_grid, cv=5, scoring='accuracy', refit=True)\n",
        "\n",
        "# Fit the GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 3. Print the best parameters and the resulting model accuracy\n",
        "# The best parameters found by the search\n",
        "print(f\"Best Parameters found by GridSearchCV: {grid_search.best_params_}\")\n",
        "\n",
        "# The best cross-validation score (accuracy)\n",
        "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate the best model on the held-out test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy of the best model on the test set: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJoJPyV2tFkC",
        "outputId": "835b7c30-586a-4166-d8be-0b4593b9991e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters found by GridSearchCV: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Best Cross-Validation Accuracy: 0.9429\n",
            "Accuracy of the best model on the test set: 1.0000\n"
          ]
        }
      ]
    }
  ]
}